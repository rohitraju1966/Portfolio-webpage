<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <title> Unmasking AI | Works</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="css/base.css">
    <link rel="stylesheet" href="css/vendor.css">
    <link rel="stylesheet" href="css/main.css">

    <!-- script
    ================================================== -->
    <script src="js/modernizr.js"></script>
    <script src="js/pace.min.js"></script>

    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="icon" href="favicon.ico" type="image/x-icon">

</head>

<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="index.html"><img src="images/frost.png" alt="Homepage" width="300"></a>
        </div>

        <nav class="header-nav-wrap">
            <ul class="header-nav">
              <li class="current"><a class="smoothscroll" href="#Introduction" title="Introduction">Introduction</a></li>
              <li><a class="smoothscroll" href="#DataPrep_EDA" title="DataPrep & EDA">DataPrep & EDA</a></li>
              <li><a class="smoothscroll" href="#Clustering" title="Clustering">Clustering</a></li>
              <li><a class="smoothscroll" href="#ARM" title="ARM">Association Rule Mining</a></li>
              <li><a class="smoothscroll" href="#LDA" title="LDA">Latent Dirichlet Allocation</a></li>
            </ul>
          </nav>
          
        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->


    <section id="home" class="s-home page-hero target-section" data-parallax="scroll" data-image-src="images/textmining/BG_image.png" data-natural-width="1600" data-natural-height="900" data-position-y="center">

        <div class="overlay"></div>
        <div class="shadow-overlay"></div>
    
        <div class="home-content">
            <div class="row home-content__main" style="display: flex; flex-direction: column; align-items: center; justify-content: center; height: 100vh; text-align: center;">
                <h1 style="color: white; font-size: 3rem;">Unmasking AI: The Tug-of-War Between Bias and Fairness</h1>
                <div class="home-content__scroll">
                    <a href="#Introduction" class="scroll-link smoothscroll">
                        <span>Scroll Down</span>
                    </a>
                </div>
            </div>
        </div> <!-- end home-content -->
    
    </section> <!-- end s-home -->
    

    <!-- about
    ================================================== -->
    <section id="Introduction" class="s-about target-section">
        <div class="row">
            <div class="row narrow section-intro has-bottom-sep">
                <div class="col-full">
                    <h3>INTRODUCTION</h3>
                </div>
            </div>
            <div class="col-full">
                <img src="images/textmining/intro_pic_2.webp" alt="AI Bias and Fairness" width="100%">
                <p>
                    Artificial Intelligence has truly introduced intelligence into the business, from leading to transforming automation to introducing pure intelligence into softwares, AI has transformed the process of how the world develops applications. Such softwares are driven mostly by instruments that can translate natural language perfectly. The advancement of AI has introduced agents which now assist professionals with activities, from automating low-level static tasks. The AI agents are also used to make predictions, to detect frauds, ad generation and making the delivery process smarter to reply back to customers automatically without spending a lot of time. It has also changed educational industries like making the curricula tailored for the students, making education more individualized, it is also being used to collaborate on learning by applying AI agents in class to improve collaborative discourse among the students. As AI continues to develop, its ability to enhance productivity and efficiency across all sectors grows immensely. The ability of such models to handle vast amounts of data improves decision-making and problem-solving. As AI becomes more involved in transforming the industry, one of the most critical questions this poses is the ethical application of AI, such as bias and fairness.
                </p>
                <p>
                    Open-source software such as fairness libraries and interpretability frameworks are becoming more common across AI development workflows so that developers can detect and repair bias early. Even while all of these promising advances are being achieved, actual fairness in AI is a monumental undertaking. Bias might be embedded in data, algorithms, or even social structure, so it shifts constantly. This is something that requires more than technical solutions, it requires ongoing monitoring, inter-disciplinary collaboration between technologists, ethicists, and policymakers, and greater public awareness and engagement. Even as AI keeps redesigning our institutions, economies, and day-to-day lives, designing fairness into the fundamental fabric of its construction is a technological as well as societal imperative. Placing equity front and center is the only way to help create public confidence and ensure such mighty technologies act in the global common interest, inclusively, fairly, and responsibly. 
                </p>
                <p>
                    The ethics of AI is the most controversial debate since there is uncertainty in its application in the industry in a just manner, it requires thorough regulation and scrutiny. Lastly, the future ability of AI to complement human intelligence will transform the work in the future. Ensuring fairness in AI is vital since unfair algorithms have the power to increase existing inequalities. Bias has a tendency to arise due to biased training data, human biases or improper model assumptions. The consequence of bias in the business world can be very severe, especially in industries such as work, lending, health and law enforcement, where an AI makes a decision that has a direct and tangible impact on individuals' lives. Governments and regulators around the globe are evolving towards coming up with laws which guarantee artificial intelligence platforms to be developed and deployed based on honor, transparency, and ethical quality. Consequently, firms are embracing strict fairness auditing, initiating advanced bias abatement techniques, and employing variety-rich and representative data so that their AI systems will become elevated in validity as well as inclusion.
                </p>
                <img src="images/textmining/intro_pic.png" alt="AI Bias and Fairness" width="100%">
                <h3>Research Questions</h3>
                <ul>
                    <li>What types of bias (gender, racial, political) appear in online discussions about AI?</li>
                    <li>How does sentiment toward AI fairness differ across various queries in the dataset?</li>
                    <li>Are specific words or phrases linked more often to certain demographic groups in discussions about AI bias?</li>
                    <li>Do users express different levels of frustration or trust in AI systems depending on the topic (fairness vs. discrimination)?</li>
                    <li>How often do discussions about AI bias reference real-world consequences (hiring, policing, education)?</li>
                    <li>Do AI-generated news articles and user-generated discussions show different perspectives on AI fairness?</li>
                    <li>Which AI-related topics (facial recognition, hiring algorithms, moderation tools) are most commonly discussed in relation to bias?</li>
                    <li>Are certain AI companies or models mentioned more frequently in bias-related discussions, and in what context?</li>
                    <li>How do discussions about AI fairness evolve over time, are certain concerns becoming more prominent?</li>
                    <li>Do different subgroups of users (technical vs. non-technical) discuss AI bias and fairness differently?</li>
                </ul>
                
            </div>
        </div>
    </section>

    <section id="DataPrep_EDA" class="s-works target-section">
        <div class="row about-content about-content--timeline">
         <div class="row narrow section-intro has-bottom-sep">
            <div class="col-full">
                <h3>DATA PREPARATION & EDA</h3>
            </div>
         </div>

        </div>

        <div class="row about-content">

            <div class="col-six tab-full left">

                <p>The data was collected through different sources by means of systematic scraping.</p>
                <p>First of all, an API request was initiated to Reddit to scrape comments related to AI bias, AI discrimination, AI fairness, and AI ethics.
                </p>
                <p> The request was initiated to comments and posts on similar Reddit pages where individuals share thoughts related to fairness and bias in AI systems openly. The information was stored along with its respective labels (400 rows), in such a way that every record was labeled according to its thematic value. Then, an API call was sent to NewsAPI to fetch news articles on the same topics of discussion (398). The aim was to gather opinions from news sources and compare them with user forums on Reddit. All the articles were also stored along with their metadata, like their label, title, description, and entire content if accessible. In the case of news and user-generated content, the dataset provides a fascinating insight into how AI bias and fairness are perceived on different platforms. To further enrich the dataset, additional data were scraped from one webpage using BeautifulSoup. The process involved scraping relevant text, stripping redundant HTML tags, and re-structuring the scraped rows to conform to the data already gathered. The newly collected entries were appended to the dataset without disrupting format and label integrity. 
                </p>
                <p> 
                    <h3>
                    Data Cleaning
                    </h3>
                    After data collection, a rigorous process of data cleaning was performed to clean the text for analysis. URLs, special characters, and unwanted symbols were removed to retain only useful textual information. The text was converted to lowercase to ensure uniformity, and stop words were removed to focus on meaningful terms. A sample raw data and its cleaned version is presented in fig. 1 and fig. 2. For normalizing word forms, stemming (refer fig. 3) and lemmatization (refer fig. 4) have been employed, shortening words back to their roots without losing their contextual meanings. Query columns of the figures are dataframe names. 
                </p>
                <img src="images/textmining/tfidf_data.png" alt="intro_1">
                <span>Fig. 5 TF-IDF Vectorized Data</span>
                <img src="images/textmining/Count_vectorized.png" alt="intro_1">
                <span>Fig. 6 Count Vectorized Data</span>
                
            </div>

            <div class="col-six tab-full right" style="text-align:center">
                <img src="images/textmining/Before_cleaning.png" alt="intro_1">
                <span>Fig. 1 Sample Data before Cleaning</span>
                <img src="images/textmining/After_cleaning.png" alt="intro_1">
                <span>Fig. 2 Sample Data after Cleaning</span>
                <img src="images/textmining/Stemmed_data.png" alt="intro_1">
                <span>Fig. 3 Stemmed Data</span>
                <img src="images/textmining/Lemetized_data.png" alt="intro_1">
                <span>Fig. 4 Lemmatized Data</span> 
                
            
            </div>
        </div>
        <div class="row">
            <p> Finally, the cleaned dataset was transformed into a format suitable for analysis. CountVectorizer (refer fig. 5) and TF-IDF vectorization (refer fig. 6) were applied to convert textual data into numerical representations to enable further analysis of word frequency and its significance in discussions on AI bias and fairness. Exploratory Data Analysis On the preprocessed data, an Exploratory Data Analysis (EDA) was conducted to uncover significant trends in the discussions on AI bias, discrimination, fairness, and ethics. The analysis involved generating TF-IDF bar charts to determine the best words in responding to each question, as well as word clouds to portray commonly occurring words in user dialogue and news articles. 
            </p>
        </div>
        <div class="row">
            <div class="col-six tab-full left">
             <h3>Exploratory Data Analysis</h3>
             <p>
                On the cleaned dataset, an Exploratory Data Analysis (EDA) was conducted to uncover key patterns in discussions surrounding AI bias, discrimination, fairness, and ethics. The analysis involved generating TF-IDF bar charts to highlight the most impactful words for each query, as well as word clouds to visualize frequently occurring terms in user discussions and news articles.
             </p>              
            <p>
                TF-IDF bar charts provided us with an understanding of the most significant words for each query and the trends in the conversation on AI-related fairness and bias. For instance, words "bias," "data," and "people" were very important when discussing AI bias (fig. 7 and fig. 8), while words "ethical," "intelligence," and "artificial" were prominent in conversations on ethical AI. In the same vein, conversations about AI fairness often highlighted "people," "think," and "make" as concerns relating to decision-making in AI (see fig. 9 and fig. 10). 
            </p>
            <img src="images/textmining/AI_Ethics_WC.png" alt="intro_1">
            <span>Fig. 10 Word Cloud representing words from 'AI Ethics' data</span>
            <p>
            <p>Word clouds also evidenced the range of conversations by extracting the most occurring words within varying themes. The use of words like "human," "system," and "work" in debates about AI bias suggests ongoing debate about the way humans interact with AI systems and their perceived fairness (see fig. 10). When debating AI ethics, words such as "think," "use," and "need" suggest the importance of decision-making and ethical considerations of AI adoption (see fig. 9). 
            </p>
            </p>
            <img src="images/textmining/AI_bias_tfidf.png" alt="intro_1">
            <span>Fig. 12 Word Cloud representing TF-IDF from 'AI Bias' data</span>
            <p style="justify-content: center;">
                <p> The TF-IDF measures of the different topics related to AI indicate distinctive patterns of word importance in different conversations. In Fig. 11, the words discrimination, artificial, and intelligence indicate significant problems in AI discrimination topics, with people and just being some of the most important words, indicating fairness-related conversation. Similarly, Fig. 12 on AI bias uses words like human, biased, and data, mirroring discussion on human influence in AI fairness and system biases. Fig. 13 on AI fairness is interestingly inclusive of French terms (dans, mais, pour), mirroring potential multilingual effects on the dataset. Lastly, Fig. 14, which is on AI ethics, has salient words like ethics, artificial, and intelligence, mirroring the significance of ethical concerns in AI systems. These findings collectively represent the nuanced discussions in AI bias, fairness, discrimination, and ethics with a strong emphasis on human influence, principles of fairness, and ethical AI deployment. </p>
            </p>
            

            </div>
            <div class="col-six tab-full right" style="text-align:center">
                <img src="images/textmining/AI_Bias_WC.png" alt="intro_1">
                <span>Fig. 7 Word Cloud representing words from 'bias' data</span>
                <img src="images/textmining/AI_Desc_WC.png" alt="intro_1">
                <span>Fig. 8 Word Cloud representing words from 'AI Discrimination' data</span>
                <img src="images/textmining/AI_fairness_WC.png" alt="intro_1">
                <span>Fig. 9 Word Cloud representing words from 'AI Fairness' data</span>
                <img src="images/textmining/AI_desc_tfidf.png" alt="intro_1">
                <span>Fig. 11 Word Cloud representing TF-IDF from 'AI Discrimination' data</span>
                <img src="images/textmining/AI_fairness_tfidf.png" alt="intro_1">
                <span>Fig. 13 Word Cloud representing TF-IDF from 'AI Ethics' data</span>
                <img src="images/textmining/AI_ethical_tfidf.png" alt="intro_1">
                <span>Fig. 14 Word Cloud representing TF-IDF from 'AI Ethics' data</span>
            </div>
        </div>
        <div class="row">
            <p>
                Overall, these visualizations provided an unambiguous snapshot of the dataset, helping identify salient themes, shared concerns, and linguistic patterns in AI discourses from various sources. 
            </p>
            <p>
                The link to the code and the data generated after cleaning can be found <a href="https://drive.google.com/drive/folders/1cTTiRxxd93waMOuB2LCg56cvMut0GTFF?usp=sharing">here.</a>
            </p>
        </div>
        
       
    </section>   

<!-- CLUSTERING Section -->
<section id="Clustering" class="s-works target-section">
    <div class="row narrow section-intro has-bottom-sep">
      <div class="col-full">
        <h4>CLUSTERING</h4>
      </div>
    </div>
    <div class="row about-content">
      <!-- Left Column: Text Content -->
      <div class="col-six tab-full left">
        <h5>Overview :</h5>
        <p>Clustering is an unsupervised machine learning method that deals with finding clusters of most similar tokens among the set of large sets of tokens in a dataset. The task of the algorithm is to use an unlabeled dataset to find clusters of similar data using distance metric like the cosine similarly or cosine distance. The two types of clustering algorithm used in this project are, Kmeans clustering and Hierarchical clustering.</p>
        
        <h5>K-means Clustering:</h5>
        <p>The algorithm behind k-means is a learning algorithm that groups data points into k clusters, given the value of k. Each cluster is represented by a centroid, which is the average or mean of all data points within that cluster. The algorithm works as follows, it works iteratively, assigning data points to the nearest centroid and then recalculating the centroids based on the new cluster assignments, it’s done using a distance metric e.g., Euclidean distance (refer to fig. 15 ) to determine the proximity of data points to cluster centroids. The main objective of k-means is to minimize the sum of squared distances between each data point and its assigned cluster centroid, eventually finding the most minimum squared distance between points and the centroid and forming clusters within the dataset.</p>
        
        <h5>Hierarchical clustering:</h5>
        <p>The algorithm behind hierarchical clustering groups data points into a hierarchy of clusters, represented as a tree-like structure (dendrogram), where clusters are eventually merged or split based on similarity. The similarity measure used in the project is the cosine similarity (refer to fig. 16). It either starts with each data point as its own cluster and iteratively merges the closest clusters until a single cluster remains, or starts with all data points in one cluster and iteratively splits it into smaller clusters until each data point is in its own cluster.</p>
        
        <p>In the current project, the work deals with using clustering to group the most similar words from the description of the new articles that talk a lot about bias and fairness in AI systems. Through clustering the algorithm is predicted to be able to clearly distinguish between the three classes of data and probably seep in and discover a fourth class to the dataset.</p>
        
        <h5>Data Prep:</h5>
        <p>Clustering as mentioned in the overview does not require labels as it works on unsupervised principles of machine learning. The dataset that is used in the current work also excludes labels as part of the data (refer to fig. 17). The dataset is also cleaned to remove stop words, special symbols, words that aren’t in english, words that aren’t part of the dictionary to obtain a cleaned dataset for processing. The cleaned text is then transformed into a numerical representation using TF-IDF (Term Frequency-Inverse Document Frequency). This method converts each document into a vector where each element represents the importance of a particular word in that document relative to the entire corpus, refer to fig. 18. Sample data before and after cleaning can be found <a href="https://drive.google.com/drive/folders/12QhJkqVIpCbYyTcLEk_SKbWmD_5QKEn0?usp=sharing">here</a></p>
        
        <h5>Code:</h5>
        <p>The following code file deals with a set of code that performs Kmeans and Hierarchical clustering on the dataset. Documents represented as TF-IDF vectors, the clustering algorithm, which is KMeans, computes the distance between these points. Euclidean distance is used on the TF-IDF vectors to determine how similar or dissimilar the documents are. Hierarchical clustering, groups data points into a hierarchy of clusters, represented as a tree-like structure (dendrogram), where clusters are eventually merged or split based on similarity. The code file can be found <a href="https://colab.research.google.com/drive/1CPM_igWSlf7G8mZx7-taRlXE3Bf7GIws?usp=sharing">here</a></p>
        
        
        
       
      </div>
      <!-- Right Column: Images -->
      <div class="col-six tab-full right" style="text-align:center">
        <img src="images/textmining/fig_15.webp" alt="fig. 15">
        <span>Fig. 15</span>
        <br>
        <img src="images/textmining/fig_16.webp" alt="fig. 16">
        <span>Fig. 16</span>
        <br>
        <img src="images/textmining/fig_17.png" alt="fig. 17">
        <span>Fig. 17</span>
        <br>
        <img src="images/textmining/fig_18.png" alt="fig. 18">
        <span>Fig. 18</span>
        <br>
        <img src="images/textmining/fig_19.png" alt="fig. 19">
        <span>Fig. 19</span>
        <br>
        <img src="images/textmining/fig_20.png" alt="fig. 20">
        <span>Fig. 20</span>
        <br>
     </div>
     <br>
    </div>
    <div class="row about-content">
      <h5>Results:</h5>
      <p>The figure 19 represents the silhouette scores that were calculated for various numbers of clusters. In our example the silhouette scores represent that the data can be best represented with three clusters as observed by a high score. To represent the data in three dimensions, PCA has been applied and the new data points are represented in three dimensions along with their cluster information, as observed in figure 20. From the figure it can be observed that the data has more or less smoothly distinguished itself into three clusters. Though the dataset was collected to represent bias and fairness, it’s fascinating to see a new class of dataset. This could potentially be the cause of outliers, it is found that the cluster represented using the purple color seems to be an outlier to the dataset, these utterances could be the ones that have nothing to do the bias and fairness as expected in any utterances.</p>
      <p>Now moving our attention to the Hierarchical clustering (hclust) results, it is observed from figure 21 that the 30 most influential words fell into two categories as expected. Observing the words that fell into each of these classes, it looks as though the model was able to distinguish between fairness and bias well, the algorithm classifying words such as ‘good’ and ‘happy’ in the first class potentially represents fairness, while ‘bias’ in the second class obviously represents ‘bias’. The most fascinating fact about the clusters is that ‘data training’ has fallen into the ‘bias’ class, indicating that often people relate training to bias, which is originally the cause of bias, and happiness being associated with fairness suggests that people often are happy about AI algorithms being fair in certain cases, indicating that fairness in AI algorithms creates happiness among humans.</p>
      <h5 >Conclusion:</h5>
      <p style="text-align:justify">From the detailed analysis conducted using various clustering algorithms, the following information was perceived. It was observed that bias and data training occurred together suggesting a close relationship between the two as expected. It is also observed that fairness in AI systems creates happiness among humans. Therefore, fairness must be the first priority while developing AI models.</p>
      <div style="text-align:center">
      <img src="images/textmining/fig_21.png" alt="fig. 21">
      <span>Fig. 21</span>
    </div>
    </div>
  </section>
  
  <!-- ASSOCIATION RULE MINING (ARM) Section -->
  <section id="ARM" class="s-works target-section">
    <div class="row narrow section-intro has-bottom-sep">
      <div class="col-full">
        <h4>ASSOCIATION RULE MINING (ARM)</h4>
      </div>
    </div>
    <div class="row about-content">
      <!-- Left Column: Text Content -->
      <div class="col-six tab-full left">
        <h5>Overview:</h5>
        <p>Association Rule Mining or ARM is another unsupervised machine learning algorithm. The logic behind ARM is the technique that discovers relationships and patterns between items in datasets. The relationship that talks about the association between items in the transaction. Given a set of items, ARM tries to find the support, confidence and lift of items occurring together. To understand more about the terms associated with ARM, here is a quick overview, 
  Support: It measures how frequently an itemset appears in the dataset.
  Confidence: Indicates how often the consequent (the "then" part of the rule) occurs when the antecedent (the "if" part) is present
  Lift: Compares the observed confidence with the expected confidence, indicating how much more likely the consequent is to occur given the antecedent.
  The formulas associated with each of the terms is as seen in Figure 22</p>
        
        <h5>Data Prep:</h5>
        <p>To perform ARM the data must be transformed into a transaction dataset. To avoid messy data, a subset of data is collected from the dataset to perform ARM. This subset of data is then cleaned and converted to transaction data, refer fig. 23 to view processed dataset and refer figure 17 to view a sample pre-processed dataset.</p>
        
        <h5>Code:</h5>
        <p>The code associated with ARM can be found <a href="https://colab.research.google.com/drive/1F1rJa9NdcgkO6EDtQu26UncImVq024pE?usp=sharing">here</a>.</p>
        
        <h5>Results:</h5>
        <p>The figures 24, 25 and 26 show the top 15 rules by support, confidence and lift respectively and figure 27 and 28 show the graphical representation of the top 15 confidence rules and top 15 lift rules respectively. From the above graphs there are rules which have 100% confidence, this is because, for the purpose of analysis a very small subset of the dataset had been used to check for the relationship between words. Therefore, the chance of two words being together at all times was higher than usual. {fairness} ⇒ {bias} (support = 0.4) This means that "fairness" and "bias" occur together in 40% of all the transactions. Confidence is around 57%, thus whenever "fairness" appears, "bias" will be present 57% of the time. {systems} ⇒ {ai} (support = 0.3) "systcms" and "ai" occur together in 30% of the transactions. Confidence is 100% here, thus every time "systems" appears, "ai" is always present in the same transaction. tech} ⇒ {bias} (confidence = 1.0) Wherever "tech" appears, "bias" lurks in that very same exchange. {algorithms} → {media} might record a 10.0 boost in the sample. This means that the co‐occurrence of "algorithms" lifts the probability of "media" by a factor of 10, compared to if "algorithms" and "media" were independent. Generally, this extremely high lift occurs when the items co‐occur together in few transactions but always together in them. {proposed} ⇒ {new} with lift = 5.0 Whenever "proposed" happens, "new" is 5 times more likely than if they were independent.</p>
        
        <h5>Conclusion:</h5>
        <p>The investigation of the synthetic bias/fairness dataset shows that certain words, such as "ai," "bias," "fairness," and "systems," occur together almost every time they are mentioned. This shows a very strong relationship between these words in the sample. However, because the dataset is controlled and limited, even a few transactions will make these associations seem unusually strong. In other words, although the results certainly indicate the intended relationships between principal concepts, the findings are influenced by the dataset's small size and makeup. For results that can be relied upon more, it would be preferable to sample a larger and more varied dataset.</p>
        <img src="images/textmining/fig_28.png" alt="fig. 28">
        <span>Fig. 28</span>  
    </div>
      <!-- Right Column: Images -->
      <div class="col-six tab-full right" style="text-align:center">
        <img src="images/textmining/fig_22.webp" alt="fig. 22">
        <span>Fig. 22</span>
        <br>
        <br>
        <img src="images/textmining/fig_23.png" alt="fig. 23">
        <span>Fig. 23</span>
        <br>
        <img src="images/textmining/fig_24.png" alt="fig. 24">
        <span>Fig. 24</span>
        <br>
        <img src="images/textmining/fig_25.png" alt="fig. 25">
        <span>Fig. 25</span>
        <br>
        <img src="images/textmining/fig_26.png" alt="fig. 26">
        <span>Fig. 26</span>
        <br>
        <img src="images/textmining/fig_27.png" alt="fig. 27">
        <span>Fig. 27</span>
        <br>
       
      </div>
    </div>
  </section>
  
  <!-- LATENT DIRICHLET ALLOCATION (LDA) Section -->
  <section id="LDA" class="s-works target-section">
    <div class="row narrow section-intro has-bottom-sep">
      <div class="col-full">
        <h4>LATENT DIRICHLET ALLOCATION (LDA)</h4>
      </div>
    </div>
    <div class="row about-content">
      <!-- Left Column: Text Content -->
      <div class="col-six tab-full left">
        <h5>Overview:</h5>
        <p>Latent Dirichlet Allocation (LDA):
  Overview:
  LDA is an unsupervised learning algorithm, the algorithm is a probabilistic topic modeling algorithm used to uncover hidden themes or topics within a collection of documents by analyzing word co-occurrence patterns. It assumes each document is a mixture of topics, and each topic is a probability distribution over words.</p>
        
        <p>LDA utilizes topic modeling, it aims to discover the underlying topics or themes in a corpus of documents (refer fig. 29).</p>
        
        <p>LDA is utilized in the work to identify patterns or topics within the dataset that most frequently occur together. Though, a fair idea has been perceived from the other two unsupervised learning techniques (ARM, clustering). LDA will help understand the most important topics being discussed in these news articles and how it relates to bias and fairness.</p>
        
        <h5>Data Prep:</h5>
        <p>First, the data is pre-processed by importing it in the lowercase form, removing punctuations and numeric values, and removing common words that do not hold much value. Then, the text is divided into individual words by CountVectorizer, and the word frequencies for each document are used to create a document-term matrix. This word-topic matrix is used as input for LDA, which goes ahead to identify the hidden topics by modeling each document as a mix of topics and each topic as a mix of words. Refer figure 30 to view a sample count vectorized dataset and figure 17 to view a sample pre-processed dataset.</p>
        
        <h5>Code:</h5>
        <p>The code file associated with LDA can be found <a href="https://colab.research.google.com/drive/1CPM_igWSlf7G8mZx7-taRlXE3Bf7GIws?usp=sharing">here</a></p>
        
        <h5>Results:</h5>
        <p>The LDA model, established to reveal four topics, produced a document-topic matrix where each document is represented as a mixture of the latent topics (see fig. 31) for each topic, the model revealed a set of important words that describe the theme. The first fifteen words of each topic were presented in horizontal bar plots (refer fig. 32, 33, 34, 35). These plots revealed that every subject was defined by a distinct collection of terms, which is indicative of the prevalence of different latent themes in the data. For example, one topic may be characterized by words of "ai" and "systems," while another may be characterized by "bias" and "fairness."</p>
        <p>A t-SNE scatter plot was used to project the document-topic matrix into two dimensions (refer fig 36). The visualization clearly showed clusters of documents, where each cluster corresponds to the dominant topic of the documents within it. The separation of the clusters indicates that the LDA model successfully uncovered distinct thematic structures in the text. Overall, the results indicate that LDA technique properly identified latent topics in the information, articulating a useful embodiment of the topical diversity in the text.</p>
        
        <h5>Conclusion:</h5>
        <p>The study identified a number of unique themes in the discussions about bias and fairness. There is a set of topics that deal with technology and its pitfalls, and a set that revolve around fairness and accountability issues. In short, the process uncovered natural clusters in the conversations, illustrating that when people talk about artificial intelligence or systems, they also talk about bias, and when they talk about fairness, they also discuss notions of equity and responsibility. This observation shows that the data represents a diversity of opinions and matters related to both technological issues and social justice, giving a clearer picture of how these topics are intertwined within the public discussion.</p>
        <img src="images/textmining/fig_36.png" alt="fig. 36">
        <span>Fig. 36</span>     
    </div>
      <!-- Right Column: Images -->
      <div class="col-six tab-full right" style="text-align:center">
        <img src="images/textmining/fig_29.webp" alt="fig. 29">
        <span>Fig. 29</span>
        <br>
        <img src="images/textmining/fig_30.png" alt="fig. 30">
        <span>Fig. 30</span>
        <br>
        <img src="images/textmining/fig_31.png" alt="fig. 31">
        <span>Fig. 31</span>
        <br>
        <img src="images/textmining/fig_32.png" alt="fig. 32">
        <span>Fig. 32</span>
        <br>
        <img src="images/textmining/fig_33.png" alt="fig. 33">
        <span>Fig. 33</span>
        <br>
        <img src="images/textmining/fig_34.png" alt="fig. 34">
        <span>Fig. 34</span>
        <br>
        <img src="images/textmining/fig_35.png" alt="fig. 35">
        <span>Fig. 35</span>
        <br>
       
      </div>
    </div>
  </section>
  
    
  
 

    <!-- s-stats
    ================================================== -->
    <section id="contact" class="s-contact target-section" >

        <div class="overlay"></div>

        <div class="row narrow section-intro">
            <div class="col-full">
                <h3>Contact</h3>
                <h1>Say Hello.</h1>
            </div>
        </div>

        <div class="row contact__main">
            <div class="col-eight tab-full contact__form">
                <form name="contactForm" id="contactForm" method="post" action="/hia">
                    <fieldset>
    
                    <div class="form-field">
                        <input name="contactName" type="text" id="contactName" placeholder="Name" value="" minlength="2" required="" aria-required="true" class="full-width">
                    </div>
                    <div class="form-field">
                        <input name="contactEmail" type="email" id="contactEmail" placeholder="Email" value="" required="" aria-required="true" class="full-width">
                    </div>
                    <div class="form-field">
                        <input name="contactSubject" type="text" id="contactSubject" placeholder="Subject" value="" class="full-width">
                    </div>
                    <div class="form-field">
                        <textarea name="contactMessage" id="contactMessage" placeholder="message" rows="10" cols="50" required="" aria-required="true" class="full-width"></textarea>
                    </div>
                    <div class="form-field">
                        <button class="full-width btn--primary">Submit</button>
                        <div class="submit-loader">
                            <div class="text-loader">Sending...</div>
                            <div class="s-loader">
                                <div class="bounce1"></div>
                                <div class="bounce2"></div>
                                <div class="bounce3"></div>
                            </div>
                        </div>
                    </div>
    
                    </fieldset>
                </form>

                <!-- contact-warning -->
                <div class="message-warning">
                    Your message was sent, thank you!<br>
                </div> 
            
                <!-- contact-success -->
                <div class="message-success">
                    Your message was sent, thank you!<br>
                </div>
                        
            </div>
            <div class="col-four tab-full contact__infos">
                <h4 class="h06">Email</h4>
                <p>Rohit.Raju@colorado.edu<br>
                </p>

                <h4 class="h06">Address</h4>
                <p>
                1600 Amphitheatre Parkway<br>
                Mountain View, CA<br>
                94043 US
                </p>
            </div>

        </div>

    </section> <!-- end s-contact -->


    <!-- footer
    ================================================== -->
    <footer>
        <div class="row">
            <div class="col-full">

                <div class="footer-logo">
                    <a class="site-logo" href="index.html"><img src="images/frost.png" alt="Homepage" width="300"></a>
                </div>

                <ul class="footer-social">

                    <li><a href="https://www.linkedin.com/in/rohitraju2001/" target=”_blank”>
                        <i class="im im-linkedin" aria-hidden="true"></i>
                        <span>Linkedin</span>
                    </a></li>
                    <li><a href="https://github.com/rohitraju1966" target=”_blank”>
                        <i class="im im-github" aria-hidden="true"></i>
                        <span>GitHub</span>
                    </a></li>
                    <li><a href="https://www.youtube.com/@FROSTtubee" target=”_blank”>
                        <i class="im im-youtube" aria-hidden="true"></i>
                        <span>Youtube</span>
                    </a></li>
                    </a></li>
                </ul>
                    
            </div>
        </div>

        <div class="row footer-bottom">

            <div class="col-twelve">
                <div class="copyright">
                    <span>© Copyright <a href="https://styleshout.com/?s=hola" target="_blank">Hola 2024</a></span> 
                    <span>Images from: <a href="https://images.google.com/" target="_blank">Google Images</a></span>
                    <span>Design by <a href="https://www.linkedin.com/in/rohitraju2001/" target=”_blank”>Rohit Raju</a></span>	
                </div>

                <div class="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
                </div>
            </div>

        </div> <!-- end footer-bottom -->

    </footer> <!-- end footer -->




    <!-- photoswipe background
    ================================================== -->
    <div aria-hidden="true" class="pswp" role="dialog" tabindex="-1">

        <div class="pswp__bg"></div>
        <div class="pswp__scroll-wrap">

            <div class="pswp__container">
                <div class="pswp__item"></div>
                <div class="pswp__item"></div>
                <div class="pswp__item"></div>
            </div>

            <div class="pswp__ui pswp__ui--hidden">
                <div class="pswp__top-bar">
                    <div class="pswp__counter"></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button> <button class="pswp__button pswp__button--share" title=
                    "Share"></button> <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button> <button class="pswp__button pswp__button--zoom" title=
                    "Zoom in/out"></button>
                    <div class="pswp__preloader">
                        <div class="pswp__preloader__icn">
                            <div class="pswp__preloader__cut">
                                <div class="pswp__preloader__donut"></div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                    <div class="pswp__share-tooltip"></div>
                </div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button> <button class="pswp__button pswp__button--arrow--right" title=
                "Next (arrow right)"></button>
                <div class="pswp__caption">
                    <div class="pswp__caption__center"></div>
                </div>
            </div>

        </div>

    </div><!-- end photoSwipe background -->

    <div id="preloader">
        <div id="loader"></div>
    </div>


    <!-- Java Script
    ================================================== -->
    <script src="js/jquery-3.2.1.min.js"></script>
    <script src="js/plugins.js"></script>
    <script src="js/main.js"></script>

</body>

</html>