<!DOCTYPE html>
<!--[if lt IE 9 ]><html class="no-js oldie" lang="en"> <![endif]-->
<!--[if IE 9 ]><html class="no-js oldie ie9" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <title> Youtube Modeling | Works</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="css/base.css">
    <link rel="stylesheet" href="css/vendor.css">
    <link rel="stylesheet" href="css/main.css">

    <!-- script
    ================================================== -->
    <script src="js/modernizr.js"></script>
    <script src="js/pace.min.js"></script>

    <!-- favicons
    ================================================== -->
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="icon" href="favicon.ico" type="image/x-icon">

</head>

<body id="top">

    <!-- header
    ================================================== -->
    <header class="s-header">

        <div class="header-logo">
            <a class="site-logo" href="index.html"><img src="images/frost.png" alt="Homepage" width="300"></a>
        </div>

        <nav class="header-nav-wrap">
            <ul class="header-nav">
                <li class="current"><a class="smoothscroll"  href="#Introduction" title="Introduction">Introduction</a></li>
                <li><a class="smoothscroll"  href="#DataPrep_EDA" title="DataPrep_ED">DataPrep_EDA</a></li>
                <li><a class="smoothscroll"  href="#Clustering" title="Clustering">Clustering</a></li>
                <li><a class="smoothscroll"  href="#ARM" title="SVMs">ARM</a></li>
                <li><a class="smoothscroll"  href="#NaiveBayes" title="Naive Bayes">Naive Bayes</a></li>
                <li><a class="smoothscroll"  href="#DecTrees" title="DecTrees">Decision Trees</a></li>
                <li><a class="smoothscroll"  href="#contact" title="Contact">Contact</a></li>
                
            </ul>
        </nav>

        <a class="header-menu-toggle" href="#0"><span>Menu</span></a>

    </header> <!-- end s-header -->


   <!-- home
   ================================================== -->
   <section id="home" class="s-home page-hero target-section" data-parallax="scroll" data-image-src="images/BG_1.jpeg" data-natural-width=1600 data-natural-height=900 data-position-y=center>

        <div class="overlay"></div>
        <div class="shadow-overlay"></div>

        <div class="home-content">

            <div class="row home-content__main">

                <div class="home-content__scroll">
                    <a href="#about" class="scroll-link smoothscroll">
                        <span>Scroll Down</span>
                    </a>
                </div>

            </div>

        </div> <!-- end home-content -->

    </section> <!-- end s-home -->


    <!-- about
    ================================================== -->
    <section id="Introduction" class="s-about target-section">
        <div class="row">
         <div class="row narrow section-intro has-bottom-sep">
             <div class="col-full">
                <h3>INTRODUCTION</h3>
                <h1>The Impact of Catchy YouTube Titles and Thumbnails on Viewer Engagement</h1>
             </div>
         </div>
            <div class="col-six tab-full left">

                <p> 
                YouTube has been the leader among content creation platforms for decades, witnessing a surge in the variety of uploaded content. YouTube demands creators to showcase their talent and optimize their content to increase the number of views, ultimately maximizing its reach. This research aims to determine the most crucial factors in boosting the number of views, with a primary focus on elements presented to users - specifically, thumbnails and titles (refer Fig. 1). While video content, watch time, likes, shares, and subscribers play significant roles in YouTube's video recommendation system, this study examines whether titles and thumbnails have a noteworthy impact on converting a video recommendation into an actual view. 
                </p>

                <p> The author's previous research establishes that titles indeed play a role in influencing views. The study observed that shorter titles gained more traction compared to longer ones. The current study aims to build upon this research by extending the idea of titles' influence on views and incorporating the characteristics of thumbnails. In the journey also explore the other performance metrics such as likes, subscribers and number of comments on a video and its relation to title and views. </p>
            </div>

            <div class="col-six tab-full right" style="text-align:center">
                <img src="images/intro_1.png" alt="intro_1">
                <span>Fig. 1 Youtube video format</span>
                <img src="images/intro_2.png" alt="intro_1">
                <span>Fig. 2 Youtube Analytics</span>
        
            </div>
        </div>
        <div class="row">
            <div>
                <p>
                    YouTube Analytics Application is a platform (Fig. 2) created by Google for YouTube content creators to analyze their data. It displays basic analytics such as the number of views, shares, likes, dislikes, subscribers, and click-through rate. Additionally, it provides content creators with the freedom to visualize their data to gain more insights into the relationship between variables. The platform offers in-depth analysis of video performance, aiding content creators in improving video quality and content. Despite these features, there is currently no tool or analytics within the platform specifically designed to help creators craft effective titles and thumbnails – a critical aspect of content creation. 
                </p>
                <p>
                    The focus of this research is to develop a tool that assists content creators in evaluating the quality of their titles and thumbnails. Additionally, the research aims to understand the impact that titles and thumbnails can have on users' mindsets, influencing their decision to either click or ignore the video. The fundamental question regarding the influence of titles and thumbnails on views leads to several intriguing research inquiries, outlined below. The project aims to shed light on the types of sentences and images that attract human attention, thereby affecting the click-through rate. This understanding could prove valuable across various domains, such as advertising, enabling brands to comprehend the impact of the text and images they employ in their advertisements and enhance them based on the findings of this research.
                </p>
                <p>
                    Such a tool could aid YouTube content creators in tailoring titles and thumbnails to any category of videos by analyzing their effectiveness in gaining traction and ultimately popularity on the platform. Though the tool does not guarantee success on YouTube, it assists in ensuring that visible aspects to the audience are tailored according to their needs, potentially increasing views. Research in the field of the impact of titles and thumbnails on views is not only important for creating a tool to enhance content creators' channel views but also for understanding the psychological impact of text and images on human behavior and its influence on video click-through rates. Understanding these psychological aspects could lead to the development of stronger advertising and marketing models.
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-full">
             <h3>Research Questions</h3>
             <p>Through this approach, the model seeks to gain insights into the human mindset, specifically addressing the question: 'What prompts a user to click on a YouTube video once recommended?' Breaking it down into 10 research questions,
              <ul><li>How does the design and content of thumbnails influence click-through rates?</li></ul>
              <ul><li>Are certain colors, images, or styles more effective in attracting clicks?</li></ul>
              <ul><li>How does the length of a video title correlate with increased views?</li></ul>
              <ul><li>To what extent does the synergy between thumbnails and titles contribute to higher view counts?</li></ul>
              <ul><li>Are there specific combinations of thumbnails and titles that perform exceptionally well?</li></ul>
              <ul><li>How do engagement metrics, such as likes, comments, and shares, correlate with the effectiveness of thumbnails and titles?</li></ul>
              <ul><li>Do the effects of thumbnails and titles on initial views extend to long-term video success?</li></ul>
              <ul><li>What is the realtion between recommendation system and impact of thumbnails and titles?</li></ul>
              <ul><li>Are there certain times of the day or week when these elements have a greater impact?</li></ul>
              <ul><li>Are there demographic factors that influence these preferences?</li></ul>
             </p>
             <p>
                The research questions stated above should give a deeper understanding of the underlying reasons behind viewer engagement and the intricate dynamics influencing the success of YouTube videos. The data collected to analyse and answer the above questions are collected from various sources. The following section will examine the data collection process,
             </p>
            </div>
        </div>
    </section>
    <section id="DataPrep_EDA" class="s-works target-section">
        <div class="row about-content about-content--timeline">
         <div class="row narrow section-intro has-bottom-sep">
            <div class="col-full">
                <h3>DATA PREPARATION & EDA</h3>
                <h1>Data Collection Process.</h1>
                
            </div>
         </div>

            <div class="col-six tab-full left">
                <div class="timeline">

                    <div class="timeline__block">
                        <div class="timeline__bullet"></div>
                        <div class="timeline__header">
                            <h3>Kaggle Data</h3>
                            <h5>Primary source of Data</h5>
                        </div>
                        <div class="timeline__desc">
                            <p>The major chuck of data that has been utilized in this project has been taken from Kaggle, uploaded by VISHWANATH SESHAGIRI <a href="https://data.world/sevenup13/youtube-video-and-channel-metadata" target=”_blank”> [source]. </a> (Click on the image below to view it in a seperate tab) </p>
                            <a href="images/CSV_1.png" target="_blank">
                                <img src="images/CSV_1.png" alt="intro_1" style="width:200px; height:200px;">
                            </a>
                        </div>
                    </div> <!-- end timeline__block -->

                    <div class="timeline__block">
                        <div class="timeline__bullet"></div>
                        <div class="timeline__header">
                            <h3>Title API request</h3>
                            <h5>API to collect title information</h5>
                        </div>
                        <div class="timeline__desc">	
                            <p><a href="https://console.cloud.google.com/apis/library" target=”_blank”>Youtube API </a> was utilized in collecting youtube title information given the video ID. Title is one of the most essential components of the data. (Click on the image below to view it in a seperate tab)</p>
                            <a href="images/CSV_2.png" target="_blank">
                                <img src="images/CSV_2.png" alt="intro_1" style="width:200px; height:200px;">
                            </a>
                        </div>
                    </div> <!-- end timeline__block -->

                </div> <!-- end timeline -->
            </div> <!-- end left -->

            <div class="col-six tab-full right">
                <div class="timeline">

                    <div class="timeline__block">
                        <div class="timeline__bullet"></div>
                        <div class="timeline__header">
                            <h3>Thumbnial URL Collection</h3>
                            <h5>API to collect thumbnail URLs</h5>
                        </div>
                        <div class="timeline__desc">
                            <p><a href="https://console.cloud.google.com/apis/library" target=”_blank”>Youtube API</a> was utilized in collecting video thumbnail URLs given the video ID. Thumbnail is another really essential components of the data <a href="https://colab.research.google.com/drive/1yFqry2OxFdPrVUTNmdsrtZUXwSgHN6o6?usp=sharing" target=”_blank”>[Source code]. </a>(Click on the image below to view it in a seperate tab)</p>
                            <a href="images/CSV_3.png" target="_blank">
                                <img src="images/CSV_3.png" alt="intro_1" style="width:200px; height:200px;">
                            </a>
                        </div>
                    </div> <!-- end timeline__block -->

                    <div class="timeline__block">
                        <div class="timeline__bullet"></div>
                        <div class="timeline__header">
                            <h3>Thumbnail Collection</h3>
                            <h5>API to collect thumbnails from URLs</h5>
                        </div>
                        <div class="timeline__desc">
                            <p>API is used to download the images from the thumbnail URLS, the downloaded thumbnails are stored into a google drive link <a href="https://drive.google.com/drive/folders/1Jh0w0j-8jwHKrQrIAONhfUd4u_erdij2?usp=sharing" target=”_blank”>[Final uncleaned data].</a> The total time taken to collect 100k titles and thumbnails was 7 days. The API data collection will continue to fetch at least 300k data points. (Click on the image below to view it in a seperate tab) </p>
                            <a href="images/CSV_4.png" target="_blank">
                                <img src="images/CSV_4.png" alt="intro_1" style="width:200px; height:200px;">
                            </a>
                        </div>
                    </div> <!-- end timeline__block -->

                </div> <!-- end timeline -->
            </div> <!-- end right -->

        </div> <!-- end about-content timeline -->
    
        <div class="row narrow section-intro has-bottom-sep">
            <div class="col-full">
                <h2>Exploratory data analysis.</h2>
                
                <p class="lead">This section will take you on a journey in understanding the underlying data and cleaning it for analysis and modeling</p>
            </div>
        </div>

        <div class="row about-content">

            <div class="col-six tab-full left">

                <p>To initiate a comprehensive understanding of the data, a word cloud (refer to Fig. 3) has been generated to provide an overview of the columns present in the dataset. It should be noted that the word cloud was created to emphasize words with higher repetition. It is evident that several 'Unnamed' columns may have been generated while storing data using an API key; these should be removed. Additionally, the 'index' column serves no purpose and can be excluded. Other columns should be retained for further analysis. Moving on to analyzing the amount of missing values in the most important columns of the data.</p>
               
            </div>

            <div class="col-six tab-full right" style="text-align:center">
                <img src="images/MLProject_3.png" alt="intro_1">
                <span>Fig. 3 Column names word cloud</span>
            
            </div>
        </div>
        <div class="row about-content">

            <div class="col-six tab-full left">
                <img src="images/MLproject_4.png" alt="intro_1">
                <p style="text-align:center">Fig. 4 Proportion of missing values in titles column</p>
                <p>After some cleaning, it is observed that none of the columns have missing values. To proceed with data preparation, a deeper examination of the columns is required. The following section will focus on the most important columns and aim to clean them, completing the data cleaning process.</p>
                <p>The video view count (Refer Fig. 6) seems to follow a exponential distribution, to understand the distributions better, the histogram for the views under 8k views is plotted. It should be noted that there is a data point that falls below 0, which is not plausible as views cannot go below 0. This issue will be resolved by removing the single row containing this anomaly.</p>
                <p>The histogram for views under 8k (refer Fig. 7) reveals that the majority of records fall within the bracket of 0 to 4000 views. To build a classification multimodal, achieving a more balanced distribution of data across various view brackets is essential. This aspect will be addressed in the upcoming sections of the research, where additional data will be collected to ensure a more even distribution.</p>
                <img src="images/MLproject_7.png" alt="intro_1">
                <p style="text-align:center">Fig. 7 Histogram for video view count (zoomed in)</p style="text-align:center">
                <p>The box plot (Refer Fig. 8), although not providing detailed information about quantiles, primarily focuses on identifying outliers. It is observed that the dataset contains numerous outliers, with some particularly significant ones having a view count exceeding 50M. Notably, there are very few videos with more than 50M views. The potential impact of these outliers on the analysis will be assessed after collecting more data to determine if this pattern persists.</p>
                <p>One of the more intriguing columns to observe is 'views/elapsedtime.' This column interprets views not merely as raw data but as a variable that changes over time. It aids in understanding whether a video gained views steadily over the long run or experienced immediate effects, and vice versa.</p>
                <img src="images/MLproject_10.png" alt="intro_1">
                <p style="text-align:center">Fig. 10 Distribution of the views/elapsed time (zoomed in)</p style="text-align:center">    
                <p>It is interesting to note that there are numerous videos with views exceeding the time elapsed by a factor of 100 to 5000 (Fig. 10). This is a notable observation, suggesting that these videos gained traction very quickly, possibly due to a higher number of subscribers on the channel. The audience's immediate interest in upcoming videos could explain the rapid viewership. However, as our focus is on assessing the impact of titles and views on overall viewership, such cases may not accurately represent the relationship. Consequently, these rows will be removed from the dataset.
                </p>
                <p>
                    The data interestingly spans across the majority of YouTube categories (Fig. 11 and Fig. 12), exhibiting an almost equal distribution among key genres such as entertainment, people and blogs, gaming, and sports. It should also be noted that most of the genres have more than 2000 vidoes in the data containing 100k data points. This diversity is beneficial, as modeling the data can provide insights into the impact of titles and thumbnails not only within a single category but across a wide spectrum of categories.
                </p>  
                <p>
                    At this point, the data is more or less clean, exhibiting no missing values and featuring appropriate data types with consistent values. While the dataset contains some outliers, they are retained at this stage as they might prove useful for the subsequent analysis. A partial snapshot of the cleaned dataset is displayed below (refer Fig. 13x). The image illustrates an additional column that has been introduced (VideoCategory), along with cleaned titles and thumbnail URL columns.
                </p>        
    
            </div>
            <div class="col-six tab-full right">
                <p>The two plots (Fig. 4 and Fig. 5) depict the number of missing values for the columns 'titles' and 'Thumbnail_URL'. It is observed that after the API data collection for 100k data points, almost 14% of the data had missing values. Removing rows with missing values is essential, as rows lacking the most important features for the research are practically not useful. The number of videos could be increased by collecting more data using the API, a step that will be taken in the further sections of the research work.</p>
                <img src="images/MLproject_5.png" alt="intro_1">
                <p style="text-align:center">Fig. 5 Proportion of missing values in thumbnail URL column</p style="text-align:center">
                <img src="images/MLproject_6.png" alt="intro_1">
                <p style="text-align:center">Fig. 6 Histogram for video view count</p style="text-align:center">
                <img src="images/MLproject_8.png" alt="intro_1">
                <p style="text-align:center">Fig. 8 Box plot of video view count</p style="text-align:center">
                <img src="images/MLproject_9.png" alt="intro_1">
                <p style="text-align:center">Fig. 9 Distribution of the views/elapsed time</p style="text-align:center">
                <p>From Fig. 9, it must be noted that most of the values lie in the range of 0 to 1, which means that most of the videos present in the data has views less than the time elapsed after posting the video. Which is obvious as majority of the videos lied in the view brackets of 0 to 4k and mostly channel elapsed time woud be greater than the number of views the video gained. But the intesting points are the ones from 100.</p>
                <img src="images/MLproject_11.png" alt="intro_1">
                <p style="text-align:center">Fig. 11 Videos per video categoryID</p style="text-align:center">
                <img src="images/MLproject_12.png" alt="intro_1">
                <p style="text-align:center">Fig. 12 Videos per video category</p style="text-align:center">
                
            </div>
        </div>
        <div class="row about-content">
            <a href="images/CSV_5.png" target="_blank">
                <img src="images/CSV_5.png" alt="intro_1" style="width:1200px; height:600px;">

            </a>
            <p style="text-align:center">Fig. 13 Cleaned dataset</p style="text-align:center">
        </div>
    </section>   
    <!-- clustering
    ================================================== -->
    <section id="Clustering" class="s-about target-section">
        <div class="row">
         <div class="row narrow section-intro has-bottom-sep">
             <div class="col-full">
                <h3>ML modeling</h3>
                <h1>K-means & Hierarchical clustering </h1>
             </div>
         </div>
            <div class="col-six tab-full left">
                <h3>Overview</h3>
                <p> 
                    From prior research, the relationship between title length and views suggests that title length does play a role in influencing the number of views a video gains. Though the relation is not directly causal, it could be said that title length does influence views. To confirm this hypothesis, k-means clustering is performed on the data. K-means clustering is a form of unsupervised learning which helps in deriving the clusters of data that are related to one another by using a distance metric. The distance metric used for this method is the Euclidean distance. Euclidean distance takes into consideration the shortest distance between data points and thus helps in finding the closest points. The formula is provided in Fig. 14. K-means clustering is a form of partition clustering which is an algorithm that divides the dataset into sub categories based on certain criteria such as the shortest distance between the clusters and the centroid. 
                </p>

                <p> 
                    To advance the analysis, titles undergo embedding using a VGG16 network (see Fig. 15) to extract key features from the text. The VGG16 network, commonly utilized for image recognition, is repurposed for text analysis. The title embeddings are then subjected to hierarchical clustering to identify the closest titles. Notably, the 'views' variable is not utilized in this method. Hierarchical clustering offers advantages over k-means clustering, as it does not require a fixed number of clusters to be specified, allowing for the natural structure of the data to emerge. The approach employs divisive clustering, where each data point initially forms a single cluster and subsequently divides into smaller clusters. The distance metric utilized for this method is cosine similarity, chosen for its effectiveness in measuring the similarity between embedding vectors (see formula in Fig 16).
                </p>
            </div>

            <div class="col-six tab-full right" style="text-align:center">
                <img src="images/Euclidean_distance.jpeg" alt="intro_1">
                <span>Fig. 14 Euclidean Distance Formula</span>
                <img src="images/VGG16_architecture.jpeg" alt="intro_1">
                <span>Fig. 15 VGG16 Architecture</span>
                <img src="images/Consine_similarity.webp" alt="intro_1">
                <span>Fig. 16 Cosine Similarity formula</span>

        
            </div>
        </div>
        <div class="row">
            <div>
                <p>
                    To determine the influence of title length on views, the 'title_length' feature is derived by counting the number of characters in the title, while the 'views/elapsed time' ratio is considered instead of views to focus on the impact of titles relative to the time since publication. The 'views/elapsedtime' column is analyzed for outliers and subsequently removed to ensure accurate clustering. Thus, for k-means clustering, a subset of the dataset containing the 'title_length' and 'views/elapsedtime' features is used (refer Fig.17). Title embeddings are obtained by first passing the titles through a pre-trained VGG16 network with its fully connected layer removed to extract key features. The resulting features are then stored in the 'title_embedding' column. These title embeddings, along with the 'views/elapsedtime' features, are used for hierarchical clustering. Although 'views/elapsedtime' is not directly used in the clustering algorithm, it is utilized post-clustering as labels to analyze the pattern that emerges from the data. The link to the sample data can be found <a href="https://docs.google.com/spreadsheets/d/1w5Jh762GTnXEW7tQVfF4yfvjZyyU_YwRO9znSFmVTco/edit?usp=sharing" target="_blank">here</a>.
                </p>
            </div>
    
         <div class="col-six tab-full left" style="text-align:center">
            <p> 
            <img src="images/K-means_data.png" alt="intro_1">
            <span>Fig. 17 Clustering Data</span>
            </p>
         </div>
         <div class="col-six tab-full right" style="text-align:center">
            <img src="images/Original_data.png" alt="intro_1">
            <span>Fig. 18 Sample of original data</span>
         </div>
        </div>
         <div class="row">
            <div>
                <h3>Code: </h3>
                <p>
                    The code to k-means clustering performed on python can be found <a href="https://colab.research.google.com/drive/1rWfW7_M6ySAT7oVtVBTT0THWUQ83A_Yu?usp=sharing" target="_blank">here</a>,
                     and the code to hierarchical clusterinf can be found <a href="https://drive.google.com/drive/folders/13dHH_yHOReKT8-NCsJOfpfMspdSEzmzm?usp=sharing" target="_blank">here</a>
                </p>
            </div>
         </div>

        <div class="row">
            <div class="col-six tab-full left">
             <h3>Results</h3>
             <p>
                <h5>The k-means clustering experiment generated the following results:</h5>
                The experiment was run on K-values ranging from 2 to 10 and the results were recorded as shown in Fig.19 and Fig. 20. K values greater than 3 generated less than 1 inertia and and gave a silhouette score less than 0.53, hence, both the silhouette score and the elbow method indicated that k=2 was the optimal value for the clustering algorithm (refer Fig. 20 and Fig. 19). The silhouette score for k=2 was obtained as 0.57, suggesting that the majority of the points belong within the clusters with a minor overlap between them. K-means clustering was conducted on 'title_length' and 'views/elapsedtime' ratio with k=2, using a boundary of title length=50 to obtain distinct clusters. The clusters were centered at (30.18, 0.40) and (62.45, 0.50), indicating a significant relationship between YouTube title length and views (refer Fig. 21).
                <div style="text-align:center"><img src="images/k_means_clusters.png" alt="intro_1" style="text-align:center"><span>Fig. 21 K-means clustering perfomed for 2 clusters</span></div>
                
             </p>              
             <p>
                <h5>The hierarchical clustering experiment generated the following results: </h5>
                <p>Three clusters were obtained while clustering the title embeddings. The first cluster primarily included titles resulting in low 'views/elapsedtime' ratios. The second cluster comprised titles with predominantly high 'views/elapsedtime' ratios. The third cluster yielded mixed results, with a combination of low and high 'views/elapsedtime' ratios.
                </p>
                <p>
                    Comparing Hierarchical and K-means clustering, it can be observed that there exist two clusters (though hierarchical provided three clusters the third one can be ignored as it does not capture the video performance based on views) and the video titles fall into each of these clusters based on its characteristics.

                 
                </p>
             </p>  
             

            </div>
            <div class="col-six tab-full right" style="text-align:center">
                <img src="images/elbow_method.png" alt="intro_1">
                <span>Fig. 19 Elbow method to find optimal K value</span>
                <img src="images/silhouette_score.png" alt="intro_1">
                <span>Fig. 20 Silhoutte method to find optimal K-value</span>
        
            </div>
            <div class="col-six tab-full right" style="text-align:center">
                <img src="images/Dendogram.png" alt="intro_1">
                <span>Fig. 22 Dendogram with hclust performed on title embeddings</span>
        
            </div>
        </div>
        <div class="row">
            <h3>Conclusion</h3>
            <p>
                Based on the results, it is observed that titles with more than 50 characters had a mean 'views/elapsedtime' of 0.5 and a mean title length of 62 characters. In contrast, titles with fewer than 50 characters displayed a mean 'views/elapsedtime' of 0.4 and a mean title length of 30 characters. This suggests that videos with shorter titles performed relatively poorly compared to videos with longer titles. It is worth noting that the boundary lies at 50 characters, and given that the maximum length of a YouTube title is 100 characters, the clusters clearly differentiate shorter titles from longer titles.
            </p>
            <p>
                The hierarchical clustering revealed three clusters based on title embeddings, each exhibiting varying views-to-elapsed-time ratios. The first cluster displayed a lower mean views-to-elapsed-time ratio, while the second cluster exhibited a higher mean ratio. The third cluster presented a mixture of views-to-elapsed-time ratios. This suggests that titles within the second cluster corresponded to videos with better performance compared to those in the other two clusters.

            </p>
            <p>
               <b> In conclusion, the results of the cluster experiment indicate that titles indeed have an impact on YouTube video performance. </b> 

            </p>
        </div>

    </section>
    <section id="ARM" class="s-works target-section">
        <div class="row about-content about-content--timeline">
         <div class="row narrow section-intro has-bottom-sep">
            <div class="col-full">
                <h3>ML modeling</h3>
                <h1>Association Rule Mining</h1>
                
            </div>
         </div>
    
        <div class="row about-content">

            <div class="col-six tab-full left">
                <h3>Overview</h3>
                <p>The conclusions drawn from clustering experiments on video titles and their impact on views suggest that titles indeed play a significant role in video performance. To deepen the analysis, association rule mining (ARM) is employed to study the relationship between title characteristics and various metrics of YouTube video performance. ARM is a method of discovering rules within data to unveil underlying relationships. These rules typically follow an 'if-then' structure, with variables on the left-hand side (LHS) as antecedents and those on the right-hand side (RHS) as consequent (refer Fig. 23)
                </p>
                <p>
                    In ARM analysis, key metrics such as support, confidence, and lift are extremely important. Support indicates the proportion of the dataset containing both the LHS and RHS of a rule, while confidence represents the conditional probability of the consequent given the antecedent. Lift measures the degree of association between the antecedent and consequent, with values greater than 1 indicating a stronger relationship than would be expected by chance.
                </p>
                <p>
                    The Apriori algorithm is used for association rule learning. It is specifically designed to discover frequent item sets in transactional databases and to generate association rules based on these frequent item sets.
                </p>
                <p>
                    This study employs apriori algorithm to investigate the relationship between title length and various metrics of video performance, including views, subscribers gained, comments received, and likes garnered. By analyzing these associations, the research aims to uncover insights into how title characteristics influence video performance outcomes.

                </p>
               
            </div>

            <div class="col-six tab-full right" style="text-align:center">
                <img src="images/Antecedent_cons.png" alt="intro_1">
                <span>Relationship between antecedent and consequent (Fig. 23)</span>
                <img src="images/AprioriSteps.png" alt="intro_1">
                <span>Apriori Algorithm (Fig. 24)</span>
            
            </div>
        </div>
        <div class="row about-content">

            <div class="col-six tab-full left">
                <h3>Data prep</h3>
                <p>
                    To prepare the data for analysis using the Apriori algorithm, it first needs to be converted into transaction data. The original data, which is in the form of Record data (refer Fig. 25), undergoes a transformation process to create transactional records suitable for association rule mining.
                </p>
                <p>
                    The transformation begins by binning the columns into categories based on an analysis of the data distribution. Each column is processed individually, with the goal of maximizing variability while maintaining equal distribution within each category. For instance, the 'title length' column is divided into 'short title' and 'long title', while the 'views/elapsed time' ratio is categorized as 'performed poorly', 'performed well', and 'performed great'.Other columns are binned as 'relatively more' or 'relatively less' compared to the median of the data. This binning process ensures that each transaction captures relevant information while reducing complexity (refer Fig. 26 for final sample dataset)
                </p>
                <p>
                    In total, approximately 125,000 transactions are generated from the original data. A sample of the final transaction data is provided <a href="https://docs.google.com/spreadsheets/d/1ZjHlWa-cRtj4xNK8GRM2QOq6D--tJRH4wNCGz4Tly4g/edit?usp=sharing" target="_blank">here</a>.
                </p>
    
            </div>
        <div class="col-six tab-full right">
            <img src="images/Original_data.png" alt="intro_1">
            <p style="text-align:center">Fig.25 Sample of Original Cleaned Data</p style="text-align:center">
            <img src="images/ARM_data.png" alt="intro_1">
            <p style="text-align:center">Fig.26 ARM data</p style="text-align:center">
        </div>
    </div>
        </div>
        <div class="row about-content">
        <p> 
            <h3>Code</h3>
            The apriori algorithm is run on ‘R’ programming language to derive the rules from the dataset. The code can be found <a href="https://drive.google.com/drive/folders/13dHH_yHOReKT8-NCsJOfpfMspdSEzmzm?usp=drive_link" target="_blank">here</a>. 
        </p>
        </div>
        <div class="row about-content">
                <h3>Results</h3>
                <p>
                    The apriori algorithm was run on the dataset to obtain top 15 rules with a fixed RHS of ‘shorter titles’ for support, confidence and lift with a minimum support of 10% and a minimum confidence of 50%. ‘Short titles’ were picked to be fixed as the RHS as it helps in exploring all the patterns that exist around short titles and its impact on the performance of the videos. 
                    The rules obtained are as follows:
                                    
                </p>
                <h5>Support</h5>
                <p>
                    <img src="images/Support.png" alt="intro_1">
                    <p style="text-align:center">Fig.27 Support Data</p style="text-align:center">
                    <img src="images/top_15_support.png" alt="intro_1">
                    <p style="text-align:center">Fig.28 Support rules Visualization</p style="text-align:center">
                </p>
                <p>
                    The top rules based on the support metric indicate that videos with shorter titles tend to perform poorly in terms of the number of subscribers, likes, and views they accumulate over time. This observation, with a support of 35%, is significant within the dataset, suggesting a substantial association between shorter titles and lower performance metrics.
                </p>
                <h5>Confidence</h5>
                <p>
                    <img src="images/Confidence.png" alt="intro_1">
                    <p style="text-align:center">Fig.29 Confidence Data</p style="text-align:center">
                    <img src="images/top_15_confidence.png" alt="intro_1">
                    <p style="text-align:center">Fig.30 Confidence rules Visualization</p style="text-align:center">
                </p>
                <p>
                    The top 15 rules obtained from confidence suggest that similar findings of support, the confidence of the rules signifies that with an average confidence of 75% it could be said that if the video performs poorly, on an average, 75% of the times it consists of a shorter title. 
                </p>
                <h5>Lift</h5>
                <p>
                    <img src="images/Lift.png" alt="intro_1">
                    <p style="text-align:center">Fig.31 Lift Data</p style="text-align:center">
                    <img src="images/top_15_lift.png" alt="intro_1">
                    <p style="text-align:center">Fig.32 Lift rules Visualization</p style="text-align:center">
                </p>
                <p>
                    The higher lift value for the rules also suggests a very strong relationship between the antecedent and the consequent, beyond what could be expected by chance. 
                </p>
                
        </div>
        <div class="row about-content">
            <h3>Conclusion</h3>
            <p>
                From the rules derived through the Apriori algorithm, a strong association emerges between <b> poor video performance and short titles </b>. Notably, an average support score of 30%, confidence level of 70%, and lift value exceeding 1 were observed for rules where poor performance metrics served as the antecedent and shorter titles as the consequent.
            </p>
            <p>
                <b>The high support, confidence, and lift values confirm that titles indeed impact video views. While the precise magnitude of this impact remains unquantified, the evident relationship between titles and video performance, as revealed by both association rule mining and clustering analyses, cannot be overlooked. 
                </b>
            </p>

        </div>
    </section>   
    <section id="NaiveBayes" class="s-works target-section">
        <div class="row about-content about-content--timeline">
         <div class="row narrow section-intro has-bottom-sep">
            <div class="col-full">
                <h3>ML modeling</h3>
                <h1>Naive Bayes Classifier</h1>
                
            </div>
         </div>
    
        <div class="row about-content">

            <div class="col-six tab-full left">
                <h3>Overview</h3>
                <p>
                    The main principle behind the Multinomial Naive Bayes algorithm is based on Bayes’ theorem (refer Fig. 33), which assists Multinomial Naive Bayes classifiers in determining the probability of a given set of features belonging to a set of classes. Multinomial Naive Bayes is used in scenarios where the data are presented in a categorical format. During training, the model learns the probability of each feature occurring in a given class. This allows it to develop a probabilistic understanding of the dataset. The probabilities calculated during training are then used during testing. This enables the model to predict the likelihood of a given set of unseen feature values and select the class with the highest probability. The Naive Bayes algorithm as a whole assumes independence between features, which is why it earned the name 'naive' Bayes. In this particular project, the Naive Bayes model is used to predict the performance categories such as 'Performed Poorly' or 'Performed Well' based on the given YouTube titles.
                </p> 
                <p>
                
                 Bernoulli Naive Bayes is another variant of the Naive Bayes algorithm specifically designed to handle binary data. It measures the presence or absence of a particular attribute in a dataset. Bernoulli Naive Bayes also makes the assumption that features are independent of one another. This means that the presence or absence of one feature does not affect the presence or absence of another feature. During training, Bernoulli Naive Bayes calculates the probability of each feature occurring or not occurring in each class. It does this by counting the number of occurrences of each feature in each class and dividing by the total number of documents in that class (refer Fig. 34).
                    
                </p>
            </div>

            <div class="col-six tab-full right" style="text-align:center">
                <img src="images/DT_NB/MNB_formula.png" alt="intro_1">
                <span>Multinomial Naive Bayes formula (Fig. 33)</span>
                <img src="images/DT_NB/BNB_formula.png" alt="intro_1">
                <span>Bernoulli Naive Bayes formula (Fig. 34 )</span>
                <img src="images/DT_NB/Laplace_Formula.png" alt="intro_1">
                <span>Laplacian Smoothing Formula (Fig. 35)</span>
                <h5>Importance of smoothing in Naive Bayes</h5>
                <p style="text-align:left">
                    Smoothing is a process aimed at preventing occurrences of zero probabilities when testing datasets. During testing, there may be instances where a particular feature has no occurrence in the training dataset, while other features may occur simultaneously. This situation would erroneously result in a zero probability for the entire test dataset, despite the chances of occurrence not being truly zero. To address this issue, various smoothing techniques such as Laplace smoothing (add-one smoothing), Lidstone smoothing, or Dirichlet smoothing are employed. By utilizing smoothing methods like Laplace smoothing, we ensure that neither the numerator nor the denominator in probability calculations becomes zero. Fig. 35 provides the formula for the most commonly used Laplace smoothing method.
                </p>
            
            </div>
        </div>
        <div class="row about-content">

            <div class="col-six tab-full left">
                <h3>Data preparation</h3>
                <p>
                    The data preparation stage of Naive Bayes was conducted in three steps,
                    <li>
                        The quantitative feature ‘views/elapsedtime’ from the cleaned dataset (Refer Fig. 36) was transformed into a qualitative feature by categorizing all values less than 0.1 as "Performed Poorly" and those greater than 0.1 as "Performed Well" (Refer Fig. 37). The value 0.1 was chosen as the cutoff point as it represents the 50th percentile of the column. 
                    </li>
                    <li>
                        The 'titles' column underwent transformation using the TF-IDF function to represent the text in a format suitable for input into the Naive Bayes algorithm. Term Frequency Inverse Document Frequency (TF-IDF) is an algorithm that assigns numerical values to each word in a sentence based on its importance within the 'titles' column. This method aids in representing the text meaningfully. (Refer Fig. 38)
                    </li>
                    <li>
                        The two transformed columns, 'views/elapsedtime' and 'titles', were subsequently extracted to form a separate data frame. This data frame was then divided into training (Refer Fig. 37 and Fig. 38) and testing sets (Refer Fig. 39). The data was partitioned into disjoint chunks for the two categories to enable training on one set of data and testing on unseen data. This approach assists in identifying potential overfitting during training, thereby aiding in the analysis of the model's capabilities. Sample test and Training Data can be found <a href="https://drive.google.com/drive/folders/1wCLZnrjDjBWjYv5P0u4Frp6p9fvqbK0d?usp=sharing" target="_blank">here</a>
                    </li>
                </p>
                <img src="images/DT_NB/Sample_X_test_NB.png" alt="intro_1">
                <p style="text-align:center">Fig.39 Test data sample</p style="text-align:center">
                <h3>Code</h3>
                <p>The code to Naive Bayes’ Classifier performed on python can be found <a href="https://colab.research.google.com/drive/1xa4Eeqxb6zmgTlQc0NeD25Z9GFJx6xu-?usp=sharing" target="_blank">here</a>. </p>
                <img src="images/DT_NB/Accuracy_NB_1.png" alt="intro_1">
                <p style="text-align:center">Fig.40 Model Evaluation Metric</p style="text-align:center">
                <img src="images/DT_NB/Accuracy_NB_2.png" alt="intro_1">
                <p style="text-align:center">Fig.41 Model Evaluation Metric</p style="text-align:center">
            </div>
            <div class="col-six tab-full right">
             <img src="images/Original_data.png" alt="intro_1">
             <p style="text-align:center">Fig.36 Sample of Original Data</p style="text-align:center">
             <img src="images/DT_NB/Sample_X_train_NB.png" alt="intro_1">
             <p style="text-align:center">Fig.37 Training data sample</p style="text-align:center">
             <img src="images/DT_NB/Sample_Y_train_NB.png" alt="intro_1">
             <p style="text-align:center">Fig.38 Training data sample</p style="text-align:center">
            </div>
    
          <div class="col-six tab-full right">
              <h3>Results</h3>
              <p>
                The Multinomial Naive Bayes model was trained on the training dataset with the alpha parameter set to 0.2 for implementing Laplace smoothing. The results were obtained by running the model on the test dataset, as shown in Figure 40. It is evident from the results that both the accuracy and precision of the model were approximately 58% for both labels. However, the recall varied slightly, with the 'Performed Poorly' label scoring 53% and the 'Performed Well' label scoring 64%. Additionally, the average accuracy, precision, recall, and F1 score for the model were approximately 58%, as depicted in Figure 41.
              </p>
              <p>
                Furthermore, analyzing the confusion matrix (Figure 42), it is evident that both labels exhibit an equal distribution throughout the matrix, indicating that the model has successfully learned both features equally.
              </p>
              <p>
                <img src="images/DT_NB/Confusion_Matrix_Naive_Baiyes_Before_smoothing.png" alt="intro_1">
                <p style="text-align:center">Fig.42 Naive Bayes Confusion Matrix</p style="text-align:center">
              </p>
          </div>
        </div>
    <div class="row about-content">
            <h3>Conclusion</h3>
            <p>
                The results suggest that while the model wasn't able to capture all the patterns between the 'titles' feature and the 'views/elapsedtime' feature from the dataset, it did demonstrate the existence of some patterns. With accuracy, precision, and recall hovering around 58%, the model performed better than random guessing. However, it falls short of being able to make meaningful predictions due to inadequate training on the dataset.
            </p>
            <p>
                <b>These outcomes align with expectations for Naive Bayes classifiers, which are basic probabilistic classifiers lacking the advanced capability to discern complex patterns present in text documents. While unsupervised learning algorithms and Naive Bayes have indicated a relationship between YouTube titles and their corresponding views, more sophisticated algorithms such as CNN or LSTM could better capture and learn intricate patterns from the dataset. These advanced models are adept at handling sequential data like text and could potentially yield more accurate predictions by extracting deeper insights from the 'titles' feature in relation to 'views/elapsedtime'.
                </b>
            </p>

    </div>
    </section>   
    <section id="DecTrees" class="s-works target-section">
        <div class="row about-content about-content--timeline">
         <div class="row narrow section-intro has-bottom-sep">
            <div class="col-full">
                <h3>ML Modeling</h3>
                <h1>Decision Trees.</h1>
                
            </div>
         </div>

        <div class="row about-content">

            <div class="col-six tab-full left">
                <h3>Overview</h3>
                <p> 
                    Decision trees (refer Figure 43) are supervised machine learning algorithms, suitable for both classification and regression tasks. The algorithm operates by partitioning the dataset's features into smaller sets with the help of input features, aiming to predict the output feature. The model trains itself by adjusting the decision of splits at each node, aiming to find the best split using either the Gini Index or Information Gain. 
                </p>
                <p>
                    <h5>What is Gini Index?</h5>
                    Gini index is a measure of the impurity present in the dataset, also referred to as entropy. In the context of decision trees, it calculates how often a randomly chosen element from the set would be incorrectly classified if it were randomly labeled according to the distribution of labels in the subset. The formula for the same is given in Fig. 44. 
                </p>
                <h5 style="text-align:left">What is Entropy ?</h5>
                <p style="text-align:left">
                    Entropy measures the amount of disorder or uncertainty in the dataset, the formula is provided in Figure 45. It is commonly used in making decisions at nodes in the decision trees. Entropy minimization is the aim while making decisions on a decision tree. Maximum entropy would mean that the classes are evenly distributed and consist of maximum disorder.
                </p>
                <img src="images/DT_NB/Entropy_formula.png" alt="intro_1">
                <p style="text-align:center">Entropy formula (Fig. 45)</p style="text-align:center">
                <img src="images/DT_NB/Information_gain.png" alt="intro_1">
                <p style="text-align:center">Information Gain formula (Fig. 46)</p style="text-align:center">
               
            </div>

            <div class="col-six tab-full right">
                <img src="images/DT_NB/DT_image.png" alt="intro_1">
                <p style="text-align:center">Decision Tree Example (Fig. 43)</p style="text-align:center">
                <img src="images/DT_NB/Gini_formula.png" alt="intro_1">
                <p style="text-align:center">Gini Index formula (Fig. 44)</p style="text-align:center">
                <h5 style="text-align:left">What is Information Gain ?</h5>
                <p style="text-align:left">
                    Information Gain quantifies the amount of information or pattern a particular subset of data would capture if it's split at that particular node, which measures the reduction in entropy achieved by a split. Entropy (refer Figure 45) measures the amount of disorder or uncertainty in the dataset. The main aim of the method is to reduce the uncertainty such that the model is able to learn patterns in the dataset. Higher the information gain the better is the split.  The formula for the same is given in Fig. 46. While this particular project aims at utilizing Gini Index as the split parameter, many applications use Information Gain and entropy to make decisions.
                </p>
                <p style="text-align:left">
                    The process of making predictions for testing is fairly simple: the tree is traversed from the root node by making decisions at each internal node based on the feature values of the data point and the splitting parameter (Gini Index in our case). The prediction is the value obtained at the final node, which is the leaf node.

                </p>
            </div>
            </div>
            <div class="row about-content">
                    <h3>Data Preparation</h3>
                    <p>
                        The data preparation stage of decision tree classifier was conducted in multiple steps,

                    </p>
                    <li>
                        The qualitative 'title' feature was transformed from the original cleaned dataset (Fig. 25) into quantitative vectors using two different embeddings: SpaCy and BERT. SpaCy, trained on large text corpora, learns various word embeddings and linguistic properties beneficial for NLP tasks like classification, named entity recognition, and part-of-speech tagging. While effective, SpaCy's capabilities are surpassed by BERT, a more advanced model trained rigorously on understanding textual nuances. BERT is commonly utilized for complex NLP tasks such as summarization, question answering, and error correction. By leveraging both SpaCy and BERT embeddings, the model gains a comprehensive understanding of the linguistic properties within the 'title' feature, facilitating accurate quantitative representation. The two embeddings are employed to evaluate their effectiveness in capturing the properties of the text documents in the project. Depending on which embedding works best for the decision trees, it could be selected as the primary embedding for further analysis.
                    </li>
                    <li>
                        Once the 'title' feature was transformed into SpaCy and BERT embeddings, The quantitative target feature ‘views/elapsedtime’ from the cleaned dataset was transformed into a qualitative feature by categorizing all values less than 0.1 as "Performed Poorly" and those greater than 0.1 as "Performed Well" (Refer Fig. 47). The value 0.1 was chosen as the cutoff point as it represents the 50th percentile of the column.
                    </li>
                    <li>
                        The two transformed columns, 'views/elapsedtime' and 'titles', were subsequently extracted to form two separate data frames—one containing SpaCy embeddings and the other containing BERT embeddings. These data frames were then divided into training (80%) (Refer Fig. 47) and testing sets (20%) (Refer Fig. 48). The data was partitioned into disjoint chunks for the two categories to facilitate training on one set of data and testing on unseen data. This approach helps identify potential overfitting during training, thereby aiding in the analysis of the model's capabilities. Sample test and Training Data can be found <a href="https://drive.google.com/drive/folders/1wCLZnrjDjBWjYv5P0u4Frp6p9fvqbK0d?usp=sharing" target="_blank">here</a>
                    </li>
                    <p> Click on each of the images below to view the sample dataset in each category. </p>
                    <img src="images/Original_data.png" alt="intro_1">
                    <p style="text-align:center">Fig.25 Sample of Original Cleaned Data</p style="text-align:center">
                     
            </div>
            <div class="col-six tab-full left">
                <div class="timeline">

                    <div class="timeline__block">
                        <div class="timeline__bullet"></div>
                        <div class="timeline__header">
                            <h3>Train Data</h3>
                            <h5>BERT Embeddings (Fig. 47)</h5>
                        </div>
                        <div class="timeline__desc">
                            <a href="images/DT_NB/Sample_X_train_BERT_DT.png" target="_blank">
                                <img src="images/DT_NB/Sample_X_train_BERT_DT.png" alt="intro_1" style="width:200px; height:350px;">
                            </a>
                            <a href="images/DT_NB/Sample_Y_train_DT.png" target="_blank">
                                <img src="images/DT_NB/Sample_Y_train_DT.png" alt="intro_1" style="width:200px; height:350px;">
                            </a>
                        </div>
                    </div> <!-- end timeline__block -->

                    <div class="timeline__block">
                        <div class="timeline__bullet"></div>
                        <div class="timeline__header">
                            <h3>Train Data</h3>
                            <h5>SpaCy Embeddings (Fig. 47)</h5>
                        </div>
                        <div class="timeline__desc">	
                            <a href="images/DT_NB/Sample_X_train_SpaCy_DT.png" target="_blank">
                                <img src="images/DT_NB/Sample_X_train_SpaCy_DT.png" alt="intro_1" style="width:280px; height:350px;">
                            </a>
                            <a href="images/DT_NB/Sample_Y_train_DT.png" target="_blank">
                                <img src="images/DT_NB/Sample_Y_train_DT.png" alt="intro_1" style="width:120px; height:350px;">
                            </a>
                        </div>
                    </div> <!-- end timeline__block -->

                </div> <!-- end timeline -->
            </div> <!-- end left -->

            <div class="col-six tab-full right">
                <div class="timeline">

                    <div class="timeline__block">
                        <div class="timeline__bullet"></div>
                        <div class="timeline__header">
                            <h3>Test Data</h3>
                            <h5>BERT Embeddings (Fig. 48)</h5>
                        </div>
                        <div class="timeline__desc">
                            <a href="images/DT_NB/Sample_X_test_BERT_DT.png" target="_blank">
                                <img src="images/DT_NB/Sample_X_test_BERT_DT.png" alt="intro_1" style="width:200px; height:350px;">
                            </a>
                            <a href="images/DT_NB/Sample_Y_test_DT.png" target="_blank">
                                <img src="images/DT_NB/Sample_Y_test_DT.png" alt="intro_1" style="width:200px; height:350px;">
                            </a>
                        </div>
                    </div> <!-- end timeline__block -->

                    <div class="timeline__block">
                        <div class="timeline__bullet"></div>
                        <div class="timeline__header">
                            <h3>Test Data</h3>
                            <h5>SpaCy Embeddings (Fig. 48)</h5>
                        </div>
                        <div class="timeline__desc">
                            <a href="images/DT_NB/Sample_X_train_BERT_DT.png" target="_blank">
                                <img src="images/DT_NB/Sample_X_test_BERT_DT.png" alt="intro_1" style="width:200px; height:350px;">
                            </a>
                            <a href="images/DT_NB/Sample_Y_train_DT.png" target="_blank">
                                <img src="images/DT_NB/Sample_Y_test_DT.png" alt="intro_1" style="width:200px; height:350px;">
                            </a>
                        </div>
                    </div> <!-- end timeline__block -->

                </div> <!-- end timeline -->
            </div> <!-- end right -->
            <h3>Code</h3>
            <div>
                
                <p> 
                    The code to Decision Tree Classifier performed on python can be found here: <a href="https://colab.research.google.com/drive/1wJz5Jrl1mB8cSsF6oMhF869BbMvpQw3o?usp=sharing" target="_blank">SpaCy</a>, <a href=" https://colab.research.google.com/drive/1xa4Eeqxb6zmgTlQc0NeD25Z9GFJx6xu-?usp=sharing" target="_blank">BERT</a>
                </p>

            </div>
            <div class="row about-content">
                

                <div class="col-six tab-full left">
                    <h3>Results</h3>
                    <p> 
                        The Decision Tree Model was trained on the training dataset, and the results were obtained from the testing dataset and visualized in Figure 49. Analysis of the visualization reveals that both the accuracy and recall metrics, derived from both BERT and SpaCy embeddings, achieved scores in the vicinity of 55%. Initially trained with the default 'max_depth' parameter, the model exhibited signs of overfitting, evident from the complex tree structure observed in Figure 50. However, upon adjusting the 'max_depth' parameter to 5, the model demonstrated improved fitting to the dataset, as depicted in Figure 51 (a) and Figure 51 (b).
                    </p>
    
                </div>
    
                <div class="col-six tab-full right">
                    <img src="images/DT_NB/DT_accuracy_Recall.png" alt="intro_1">
                    <p style="text-align:center">Decision Tree evaluation metric (Fig. 49)</p style="text-align:center">
                </div>
                    <img src="images/DT_NB/decision_tree_plot_Maxdepth10.png" alt="intro_1">
                    <p style="text-align:center">Initial Decision tree (Fig. 50)</p style="text-align:center"> 
                    <img src="images/DT_NB/Decision_tree_plot_BERT.png" alt="intro_1">
                    <p style="text-align:center">Decision Tree using BERT Embeddings (Fig. 51 (a))</p style="text-align:center">
                    <img src="images/DT_NB/Decision_tree_SpaCy.png" alt="intro_1">
                    <p style="text-align:center">Decision Tree using SpaCy Embeddings (Fig. 51 (b))</p style="text-align:center">
                    <p>
                            Furthermore, examination of the confusion matrix, Fig. 52 (a) and Fig. 52 (b) from both embeddings indicates an even distribution of both labels throughout the matrix. This uniform distribution suggests that the model effectively learned both features with equal proficiency.
                    </p>
                    <img src="images/DT_NB/Confusion_matrix_BERT_DT.png" alt="intro_1" style="width:400px; height:350px;">
                    <img src="images/DT_NB/Confusion_matrix_Spacy.png" alt="intro_1" style="width:400px; height:350px;">
                    <p>Confusion Matrix for BERT and SpaCy Embeddings DT (Fig. 52 (a) and Fig. 52 (b))</p>   
                    <p>
                        <h3>Conclusion</h3>
                        <p>In conclusion, the experiment revealed that the decision tree classifier struggled to effectively train on the dataset due to its complexity. Nonetheless, the findings suggest the presence of a discernible pattern between YouTube video titles and their corresponding views over time. Although setting the 'max_depth' parameter to 5 helped prevent overfitting, it did not significantly improve model accuracy on the test dataset.
                        </p>
                        <p>
                            The experiment, conducted using both BERT and SpaCy embeddings, aimed to observe any differences in predictions. Surprisingly, both embeddings performed similarly, indicating that the decision tree classifier may not effectively leverage the advanced linguistic characteristics captured by BERT. <b>This suggests that the decision tree classifier may not be the optimal model for capturing the complex patterns within text documents. Moving forward, it is evident that employing more advanced models like CNN or LSTMs may yield better classification results by effectively leveraging the intricate linguistic features present in the text. </b>
                        </p>
                    </p>
                </div>


        </div> <!-- end about-content timeline -->
    
    </section>



    <!-- s-stats
    ================================================== -->
    <section id="contact" class="s-contact target-section" >

        <div class="overlay"></div>

        <div class="row narrow section-intro">
            <div class="col-full">
                <h3>Contact</h3>
                <h1>Say Hello.</h1>
            </div>
        </div>

        <div class="row contact__main">
            <div class="col-eight tab-full contact__form">
                <form name="contactForm" id="contactForm" method="post" action="/hia">
                    <fieldset>
    
                    <div class="form-field">
                        <input name="contactName" type="text" id="contactName" placeholder="Name" value="" minlength="2" required="" aria-required="true" class="full-width">
                    </div>
                    <div class="form-field">
                        <input name="contactEmail" type="email" id="contactEmail" placeholder="Email" value="" required="" aria-required="true" class="full-width">
                    </div>
                    <div class="form-field">
                        <input name="contactSubject" type="text" id="contactSubject" placeholder="Subject" value="" class="full-width">
                    </div>
                    <div class="form-field">
                        <textarea name="contactMessage" id="contactMessage" placeholder="message" rows="10" cols="50" required="" aria-required="true" class="full-width"></textarea>
                    </div>
                    <div class="form-field">
                        <button class="full-width btn--primary">Submit</button>
                        <div class="submit-loader">
                            <div class="text-loader">Sending...</div>
                            <div class="s-loader">
                                <div class="bounce1"></div>
                                <div class="bounce2"></div>
                                <div class="bounce3"></div>
                            </div>
                        </div>
                    </div>
    
                    </fieldset>
                </form>

                <!-- contact-warning -->
                <div class="message-warning">
                    Your message was sent, thank you!<br>
                </div> 
            
                <!-- contact-success -->
                <div class="message-success">
                    Your message was sent, thank you!<br>
                </div>
                        
            </div>
            <div class="col-four tab-full contact__infos">
                <h4 class="h06">Email</h4>
                <p>Rohit.Raju@colorado.edu<br>
                </p>

                <h4 class="h06">Address</h4>
                <p>
                1600 Amphitheatre Parkway<br>
                Mountain View, CA<br>
                94043 US
                </p>
            </div>

        </div>

    </section> <!-- end s-contact -->


    <!-- footer
    ================================================== -->
    <footer>
        <div class="row">
            <div class="col-full">

                <div class="footer-logo">
                    <a class="site-logo" href="index.html"><img src="images/frost.png" alt="Homepage" width="300"></a>
                </div>

                <ul class="footer-social">

                    <li><a href="https://www.linkedin.com/in/rohit-r-0a61a5201/" target=”_blank”>
                        <i class="im im-linkedin" aria-hidden="true"></i>
                        <span>Linkedin</span>
                    </a></li>
                    <li><a href="https://www.youtube.com/@FROSTtubee" target=”_blank”>
                        <i class="im im-youtube" aria-hidden="true"></i>
                        <span>Youtube</span>
                    </a></li>
                    <li><a href="https://www.instagram.com/thefrostube/" target=”_blank”>
                        <i class="im im-instagram" aria-hidden="true"></i>
                        <span>Instagram</span>
                    </a></li>
                    </a></li>
                </ul>
                    
            </div>
        </div>

        <div class="row footer-bottom">

            <div class="col-twelve">
                <div class="copyright">
                    <span>© Copyright <a href="https://styleshout.com/?s=hola" target="_blank">Hola 2024</a></span> 
                    <span>Images from: <a href="https://images.google.com/" target="_blank">Google Images</a></span>
                    <span>Design by <a href="https://www.linkedin.com/in/rohit-r-0a61a5201/" target=”_blank”>Rohit Raju</a></span>	
                </div>

                <div class="go-top">
                <a class="smoothscroll" title="Back to Top" href="#top"><i class="im im-arrow-up" aria-hidden="true"></i></a>
                </div>
            </div>

        </div> <!-- end footer-bottom -->

    </footer> <!-- end footer -->




    <!-- photoswipe background
    ================================================== -->
    <div aria-hidden="true" class="pswp" role="dialog" tabindex="-1">

        <div class="pswp__bg"></div>
        <div class="pswp__scroll-wrap">

            <div class="pswp__container">
                <div class="pswp__item"></div>
                <div class="pswp__item"></div>
                <div class="pswp__item"></div>
            </div>

            <div class="pswp__ui pswp__ui--hidden">
                <div class="pswp__top-bar">
                    <div class="pswp__counter"></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button> <button class="pswp__button pswp__button--share" title=
                    "Share"></button> <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button> <button class="pswp__button pswp__button--zoom" title=
                    "Zoom in/out"></button>
                    <div class="pswp__preloader">
                        <div class="pswp__preloader__icn">
                            <div class="pswp__preloader__cut">
                                <div class="pswp__preloader__donut"></div>
                            </div>
                        </div>
                    </div>
                </div>
                <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                    <div class="pswp__share-tooltip"></div>
                </div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"></button> <button class="pswp__button pswp__button--arrow--right" title=
                "Next (arrow right)"></button>
                <div class="pswp__caption">
                    <div class="pswp__caption__center"></div>
                </div>
            </div>

        </div>

    </div><!-- end photoSwipe background -->

    <div id="preloader">
        <div id="loader"></div>
    </div>


    <!-- Java Script
    ================================================== -->
    <script src="js/jquery-3.2.1.min.js"></script>
    <script src="js/plugins.js"></script>
    <script src="js/main.js"></script>

</body>

</html>